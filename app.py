import streamlit as st
import pandas as pd
import numpy as np
from polyfuzz import PolyFuzz
import time
import io
import gc
from typing import List, Dict, Tuple, Optional, Union
import openai
import json
import re
from urllib.parse import urlparse
import openpyxl
import psutil
import os
import math

# Configure page
st.set_page_config(
    page_title="URL Migration Mapping Tool",
    page_icon="üîó",
    layout="wide",
    initial_sidebar_state="expanded"
)

class URLMigrationMapper:
    def __init__(self):
        self.df_live = None
        self.df_staging = None
        self.openai_client = None
        self.required_columns = {
            'pre': ['Address', 'Status Code'],
            'post': ['Address', 'Status Code', 'Indexability']
        }
        self.optional_columns = ['Title 1', 'H1-1']
        self.batch_size = 5000
        self.max_memory_usage = 85
        
    def get_memory_usage(self) -> float:
        """Get current memory usage percentage"""
        return psutil.virtual_memory().percent
    
    def calculate_optimal_batch_size(self, total_rows: int, available_memory_gb: float = None) -> int:
        """Calculate optimal batch size based on data size and available memory"""
        if available_memory_gb is None:
            available_memory_gb = psutil.virtual_memory().available / (1024**3)
        
        memory_per_row_mb = 0.001
        max_rows_in_memory = int((available_memory_gb * 1024 * 0.5) / memory_per_row_mb)
        
        if total_rows <= max_rows_in_memory:
            return total_rows
        
        num_batches = math.ceil(total_rows / max_rows_in_memory)
        optimal_batch_size = math.ceil(total_rows / num_batches)
        optimal_batch_size = max(1000, min(optimal_batch_size, 10000))
        
        return optimal_batch_size
    
    def setup_openai(self, api_key: str) -> bool:
        """Setup OpenAI client"""
        try:
            self.openai_client = openai.OpenAI(api_key=api_key)
            self.openai_client.models.list()
            return True
        except Exception as e:
            st.error(f"Errore nella configurazione OpenAI: {str(e)}")
            return False
    
    def load_file_chunked(self, uploaded_file, chunk_size: int = 10000) -> pd.DataFrame:
        """Load large files in chunks to avoid memory issues"""
        try:
            file_size_mb = uploaded_file.size / (1024 * 1024)
            st.info(f"üìÅ Caricamento file: {uploaded_file.name} ({file_size_mb:.1f} MB)")
            
            if uploaded_file.name.endswith('.csv'):
                # Enhanced CSV parameters for problematic files
                csv_params = {
                    'on_bad_lines': 'skip',  # Skip bad lines instead of warning (faster)
                    'sep': ',',
                    'quotechar': '"',
                    'skipinitialspace': True,
                    'low_memory': False,
                    'encoding': 'utf-8',
                    'engine': 'python',     # More flexible parser
                    'error_bad_lines': False  # Don't raise errors on bad lines
                }
                
                # Try different encodings
                encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
                
                if file_size_mb > 100:  # Large file handling
                    st.info("üìä File grande rilevato - utilizzo caricamento ottimizzato con gestione errori")
                    
                    chunks = []
                    df_loaded = False
                    skipped_lines = 0
                    
                    for encoding in encodings_to_try:
                        try:
                            csv_params['encoding'] = encoding
                            
                            # Reset file pointer
                            uploaded_file.seek(0)
                            
                            # Use smaller chunks for problematic files
                            safe_chunk_size = min(chunk_size, 5000) if file_size_mb > 500 else chunk_size
                            
                            chunk_iterator = pd.read_csv(uploaded_file, 
                                                       chunksize=safe_chunk_size, 
                                                       **csv_params)
                            
                            total_chunks = math.ceil(file_size_mb / 5)  # More conservative estimate
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            
                            chunk_count = 0
                            for i, chunk in enumerate(chunk_iterator):
                                # Memory check before processing each chunk
                                current_memory = self.get_memory_usage()
                                if current_memory > self.max_memory_usage:
                                    st.warning(f"‚ö†Ô∏è Memoria alta ({current_memory:.1f}%), consolidando chunks...")
                                    if chunks:  # Only break if we have some data
                                        break
                                
                                # Clean the chunk
                                if not chunk.empty:
                                    # Remove completely empty rows
                                    chunk = chunk.dropna(how='all')
                                    
                                    if not chunk.empty:
                                        chunks.append(chunk)
                                        chunk_count += 1
                                
                                # Update progress
                                progress = min((i + 1) / total_chunks, 1.0)
                                progress_bar.progress(progress)
                                status_text.text(f"üì¶ Processati {chunk_count} chunks, memoria: {current_memory:.1f}%")
                                
                                # Periodic garbage collection for large files
                                if i % 10 == 0:
                                    gc.collect()
                            
                            if chunks:
                                status_text.text("üîó Combinando chunks...")
                                df = pd.concat(chunks, ignore_index=True, sort=False)
                                del chunks
                                gc.collect()
                                df_loaded = True
                                st.success(f"‚úÖ File caricato con encoding: {encoding} ({chunk_count} chunks)")
                                break
                            else:
                                st.warning(f"‚ö†Ô∏è Nessun dato valido trovato con encoding {encoding}")
                                continue
                            
                        except Exception as e:
                            if chunks:
                                del chunks
                                chunks = []
                            st.warning(f"‚ö†Ô∏è Errore con encoding {encoding}: {str(e)[:100]}...")
                            continue
                    
                    if not df_loaded:
                        raise ValueError("Impossibile caricare il file. Troppi errori nei dati.")
                        
                else:  # Smaller files
                    df_loaded = False
                    
                    for encoding in encodings_to_try:
                        try:
                            csv_params['encoding'] = encoding
                            uploaded_file.seek(0)
                            df = pd.read_csv(uploaded_file, **csv_params)
                            df_loaded = True
                            st.success(f"‚úÖ File caricato con encoding: {encoding}")
                            break
                            
                        except Exception as e:
                            st.warning(f"‚ö†Ô∏è Errore con encoding {encoding}: {str(e)[:100]}...")
                            continue
                    
                    if not df_loaded:
                        raise ValueError("Impossibile caricare il file con nessuno degli encoding testati")
                    
            elif uploaded_file.name.endswith(('.xlsx', '.xls')):
                # Excel files with better error handling
                if file_size_mb > 50:
                    st.warning("‚ö†Ô∏è File Excel grande rilevato. Il caricamento potrebbe richiedere tempo...")
                
                try:
                    df = pd.read_excel(uploaded_file, engine='openpyxl')
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Tentativo con openpyxl fallito: {str(e)}")
                    try:
                        df = pd.read_excel(uploaded_file, engine='xlrd')
                    except Exception as e2:
                        raise ValueError(f"Impossibile caricare il file Excel: {str(e2)}")
            else:
                raise ValueError("Formato file non supportato")
            
            # Enhanced post-processing
            st.info(f"üîç Ottimizzazione file caricato...")
            
            original_rows = len(df)
            original_columns = len(df.columns)
            
            # Remove completely empty rows
            df = df.dropna(how='all')
            empty_rows_removed = original_rows - len(df)
            
            if empty_rows_removed > 0:
                st.info(f"üóëÔ∏è Rimosse {empty_rows_removed} righe completamente vuote")
            
            # Remove completely empty columns
            empty_cols = df.columns[df.isnull().all()].tolist()
            if empty_cols:
                df.drop(columns=empty_cols, inplace=True)
                st.info(f"üóëÔ∏è Rimosse {len(empty_cols)} colonne completamente vuote")
            
            # Handle unnamed columns more intelligently
            unnamed_cols = [col for col in df.columns if str(col).startswith('Unnamed:')]
            if unnamed_cols:
                cols_to_remove = []
                for col in unnamed_cols:
                    non_null_pct = df[col].notna().sum() / len(df)
                    if non_null_pct < 0.05:  # Less than 5% non-null
                        cols_to_remove.append(col)
                
                if cols_to_remove:
                    df.drop(columns=cols_to_remove, inplace=True)
                    st.info(f"üóëÔ∏è Rimosse {len(cols_to_remove)} colonne 'Unnamed' quasi vuote")
            
            # Clean column names
            df.columns = df.columns.astype(str)
            df.columns = [col.strip().replace('\n', ' ').replace('\r', ' ') for col in df.columns]
            
            # Memory optimization for large files
            if len(df) > 50000:
                st.info("üîß Ottimizzazione tipi di dati per file grande...")
                
                # Optimize string columns
                for col in df.columns:
                    if df[col].dtype == 'object':
                        # Try to convert to category if many repeated values
                        unique_ratio = df[col].nunique() / len(df)
                        if unique_ratio < 0.5:  # Less than 50% unique values
                            try:
                                df[col] = df[col].astype('category')
                            except:
                                df[col] = df[col].astype('string')
                        else:
                            df[col] = df[col].astype('string')
                
                gc.collect()
            
            # Final stats
            final_rows = len(df)
            final_columns = len(df.columns)
            
            st.success(f"‚úÖ File ottimizzato: {final_rows:,} righe, {final_columns} colonne")
            
            if final_rows != original_rows or final_columns != original_columns:
                st.info(f"üìä Ottimizzazioni: {original_rows:,}‚Üí{final_rows:,} righe, {original_columns}‚Üí{final_columns} colonne")
            
            # Show column preview
            if len(df.columns) > 15:
                st.info(f"üîç Prime 15 colonne: {list(df.columns[:15])}")
                st.info(f"üìù + altre {len(df.columns) - 15} colonne...")
            else:
                st.info(f"üîç Colonne: {list(df.columns)}")
            
            # Memory usage info
            memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
            st.info(f"üíæ Utilizzo memoria DataFrame: {memory_usage:.1f} MB")
            
            return df
            
        except Exception as e:
            error_msg = str(e)
            st.error(f"‚ùå Errore nel caricamento del file {uploaded_file.name}")
            st.error(f"Dettaglio errore: {error_msg}")
            
            # Provide specific suggestions based on error type
            if "expected" in error_msg and "saw" in error_msg:
                st.markdown("""
                **üîß Problema rilevato: Colonne inconsistenti**
                
                **Soluzioni consigliate:**
                1. **Apri il file in Excel** e verifica che tutte le righe abbiano lo stesso numero di colonne
                2. **Cerca righe con virgole extra** nei contenuti (es. descrizioni, title)
                3. **Salva come CSV UTF-8** da Excel per pulire il formato
                4. **Rimuovi manualmente** le righe problematiche (vedi numeri sopra)
                """)
            elif "encoding" in error_msg.lower():
                st.markdown("""
                **üîß Problema rilevato: Encoding del file**
                
                **Soluzioni:**
                - Salva il file come **CSV UTF-8** da Excel
                - Prova a convertire con un editor di testo (Notepad++, VS Code)
                """)
            elif "memory" in error_msg.lower():
                st.markdown("""
                **üîß Problema rilevato: Memoria insufficiente**
                
                **Soluzioni:**
                - Dividi il file in parti pi√π piccole
                - Rimuovi colonne non necessarie prima del caricamento
                - Chiudi altre applicazioni per liberare memoria
                """)
            else:
                st.markdown("""
                **üí° Suggerimenti generali:**
                - Verifica che il file non sia corrotto
                - Prova a salvarlo nuovamente come CSV UTF-8
                - Controlla che non ci siano caratteri speciali nei dati
                - Se il file √® molto grande, considera di dividerlo
                """)
            
            return None
    
    def monitor_system_health(self) -> bool:
        """Monitor system health and return False if resources are critically low"""
        try:
            memory_percent = psutil.virtual_memory().percent
            
            if memory_percent > 95:
                st.error(f"üö® MEMORIA CRITICA: {memory_percent:.1f}% - Interrompendo elaborazione per sicurezza")
                return False
            elif memory_percent > 90:
                st.warning(f"‚ö†Ô∏è MEMORIA ALTA: {memory_percent:.1f}% - Procedendo con cautela")
                gc.collect()  # Force garbage collection
                return True
            
            return True
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Errore monitoraggio sistema: {str(e)}")
            return True  # Continue if monitoring fails
    
    def safe_processing_wrapper(self, func, *args, **kwargs):
        """Wrapper to safely execute processing functions with health monitoring"""
        try:
            # Check system health before starting
            if not self.monitor_system_health():
                raise MemoryError("Sistema in stato critico - elaborazione interrotta")
            
            # Execute the function
            result = func(*args, **kwargs)
            
            # Check system health after execution
            if not self.monitor_system_health():
                st.warning("‚ö†Ô∏è Memoria alta dopo elaborazione - liberando risorse...")
                gc.collect()
            
            return result
            
        except MemoryError as e:
            st.error(f"‚ùå Errore memoria: {str(e)}")
            st.info("üí° Prova a ridurre le dimensioni del file o chiudere altre applicazioni")
            gc.collect()
            raise
        except Exception as e:
            st.error(f"‚ùå Errore durante l'elaborazione: {str(e)}")
            gc.collect()
            raise
        """Load CSV or XLSX file with automatic chunking for large files"""
        return self.load_file_chunked(uploaded_file)
    
    def validate_columns(self, df: pd.DataFrame, file_type: str, selected_columns: List[str]) -> Tuple[bool, List[str]]:
        """Validate required columns exist"""
        required = self.required_columns[file_type]
        all_required = required + selected_columns
        missing = [col for col in all_required if col not in df.columns]
        
        if missing:
            return False, missing
        return True, []
    
    def clean_duplicate_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Remove duplicate column names and handle conflicts"""
        if len(df.columns) != len(set(df.columns)):
            st.warning("‚ö†Ô∏è Colonne duplicate rilevate, pulizia in corso...")
            
            # Create a mapping of old to new column names
            new_columns = []
            seen_columns = {}
            
            for col in df.columns:
                if col in seen_columns:
                    seen_columns[col] += 1
                    new_col = f"{col}_duplicate_{seen_columns[col]}"
                else:
                    seen_columns[col] = 0
                    new_col = col
                new_columns.append(new_col)
            
            # Assign new column names
            df.columns = new_columns
            
            # Remove obviously duplicate Address columns (keep only the main one)
            duplicate_address_cols = [col for col in df.columns if col.startswith('Address_duplicate_')]
            if duplicate_address_cols:
                df.drop(columns=duplicate_address_cols, inplace=True, errors='ignore')
                st.info(f"üóëÔ∏è Rimosse colonne duplicate: {duplicate_address_cols}")
            
            st.info(f"‚úÖ Colonne pulite: {len(df.columns)} colonne rimanenti")
        
        return df

    def ensure_unique_columns(self, df: pd.DataFrame, operation_name: str = "") -> pd.DataFrame:
        """Ensure all columns have unique names before operations"""
        if len(df.columns) != len(set(df.columns)):
            st.warning(f"‚ö†Ô∏è Colonne duplicate trovate prima di {operation_name}. Correzione in corso...")
            
            # Make columns unique by adding suffix
            cols = pd.Series(df.columns)
            for dup in cols[cols.duplicated()].unique():
                mask = cols == dup
                cols[mask] = [f"{dup}_v{i}" if i != 0 else dup for i in range(mask.sum())]
            
            df.columns = cols
            
            st.info(f"‚úÖ Colonne rese uniche per {operation_name}")
        
        return df
    
    def preprocess_data_batch(self, df_live: pd.DataFrame, df_staging: pd.DataFrame, 
                             matching_columns: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """Preprocess the data with memory optimization"""
        st.write("üîÑ Preprocessing dei dati...")
        
        df_live = df_live.copy()
        df_staging = df_staging.copy()
        
        # Clean duplicate columns FIRST - this is critical
        df_live = self.ensure_unique_columns(df_live, "preprocessing df_live")
        df_staging = self.ensure_unique_columns(df_staging, "preprocessing df_staging")
        
        # Fix Status Code data type - convert to numeric
        if 'Status Code' in df_live.columns:
            try:
                # Convert to numeric, replacing non-numeric values with NaN
                df_live['Status Code'] = pd.to_numeric(df_live['Status Code'], errors='coerce')
                # Fill NaN values with 200 (default OK status)
                df_live['Status Code'] = df_live['Status Code'].fillna(200).astype(int)
                st.info("üîß Convertita colonna 'Status Code' (PRE) in formato numerico")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Errore conversione Status Code (PRE): {str(e)}")
                # If conversion fails, create a default column
                df_live['Status Code'] = 200
        
        if 'Status Code' in df_staging.columns:
            try:
                # Convert to numeric, replacing non-numeric values with NaN  
                df_staging['Status Code'] = pd.to_numeric(df_staging['Status Code'], errors='coerce')
                # Fill NaN values with 200 (default OK status)
                df_staging['Status Code'] = df_staging['Status Code'].fillna(200).astype(int)
                st.info("üîß Convertita colonna 'Status Code' (POST) in formato numerico")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Errore conversione Status Code (POST): {str(e)}")
                # If conversion fails, create a default column
                df_staging['Status Code'] = 200
        
        # Optimize data types to save memory
        for col in df_live.columns:
            if col != 'Status Code' and df_live[col].dtype == 'object':
                df_live[col] = df_live[col].astype('string')
        
        for col in df_staging.columns:
            if col != 'Status Code' and df_staging[col].dtype == 'object':
                df_staging[col] = df_staging[col].astype('string')
        
        # Ensure required columns exist
        for col in matching_columns:
            if col not in df_live.columns:
                df_live[col] = ""
            if col not in df_staging.columns:
                df_staging[col] = ""
        
        # Verify Address column exists and is unique
        if 'Address' not in df_live.columns:
            st.error("‚ùå Colonna 'Address' mancante nel file PRE")
            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
        
        if 'Address' not in df_staging.columns:
            st.error("‚ùå Colonna 'Address' mancante nel file POST")
            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()
        
        # Drop duplicates with progress tracking
        initial_live_count = len(df_live)
        initial_staging_count = len(df_staging)
        
        df_live.drop_duplicates(subset="Address", inplace=True)
        df_staging.drop_duplicates(subset="Address", inplace=True)
        
        st.write(f"üìä Duplicati rimossi: Live {initial_live_count - len(df_live)}, Staging {initial_staging_count - len(df_staging)}")
        
        # Extract non-redirectable URLs (3xx & 5xx) with safe numeric comparison
        try:
            df_3xx = df_live[(df_live['Status Code'] >= 300) & (df_live['Status Code'] <= 308)].copy()
            df_5xx = df_live[(df_live['Status Code'] >= 500) & (df_live['Status Code'] <= 599)].copy()
            df_non_redirectable = pd.concat([df_3xx, df_5xx]) if not df_3xx.empty or not df_5xx.empty else pd.DataFrame()
            
            # Keep 2xx and 4xx status codes for redirecting
            df_live_200 = df_live[(df_live['Status Code'] >= 200) & (df_live['Status Code'] <= 226)].copy()
            df_live_400 = df_live[(df_live['Status Code'] >= 400) & (df_live['Status Code'] <= 499)].copy()
            df_live_clean = pd.concat([df_live_200, df_live_400]) if not df_live_200.empty or not df_live_400.empty else pd.DataFrame()
            
            st.write(f"üìä Filtrati per status code: {len(df_live_clean):,} processabili, {len(df_non_redirectable):,} non-redirectable")
            
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Errore nel filtraggio per status code: {str(e)}")
            st.warning("üîÑ Procedendo senza filtri di status code...")
            # If status code filtering fails, use all data
            df_non_redirectable = pd.DataFrame()
            df_live_clean = df_live.copy()
        
        # Clean up intermediate dataframes
        try:
            del df_3xx, df_5xx, df_live_200, df_live_400
        except:
            pass
        gc.collect()
        
        # Handle NaN values - populate with URL for 404s
        for col in matching_columns:
            if col in df_live_clean.columns:
                df_live_clean[col] = df_live_clean[col].fillna(df_live_clean['Address'])
            if col in df_staging.columns:
                df_staging[col] = df_staging[col].fillna(df_staging['Address'])
        
        # Final check for unique columns
        df_live_clean = self.ensure_unique_columns(df_live_clean, "final df_live_clean")
        df_staging = self.ensure_unique_columns(df_staging, "final df_staging")
        
        st.write(f"‚úÖ Preprocessing completato: {len(df_live_clean):,} URL processabili, {len(df_non_redirectable):,} non-redirectable")
        
        return df_live_clean, df_staging, df_non_redirectable
    
    def perform_polyfuzz_matching_batch(self, df_live: pd.DataFrame, df_staging: pd.DataFrame, 
                                      matching_columns: List[str]) -> Dict[str, pd.DataFrame]:
        """Perform PolyFuzz matching in batches for large datasets"""
        matches = {}
        
        total_rows = max(len(df_live), len(df_staging))
        batch_size = self.calculate_optimal_batch_size(total_rows)
        
        st.write(f"üîç Avvio matching con batch size: {batch_size:,}")
        st.write(f"üíæ Memoria disponibile: {psutil.virtual_memory().available / (1024**3):.1f} GB")
        
        main_progress = st.progress(0)
        status_text = st.empty()
        
        total_operations = len(matching_columns)
        
        for i, col in enumerate(matching_columns):
            status_text.text(f"üîç Matching colonna: {col} ({i+1}/{total_operations})")
            
            if len(df_live) > batch_size or len(df_staging) > batch_size:
                all_matches = []
                
                live_batches = [df_live[j:j+batch_size] for j in range(0, len(df_live), batch_size)]
                staging_data = list(df_staging[col].astype(str))
                
                batch_progress = st.progress(0)
                
                for batch_idx, live_batch in enumerate(live_batches):
                    live_data = list(live_batch[col].astype(str))
                    
                    model = PolyFuzz("TF-IDF").match(live_data, staging_data)
                    batch_matches = model.get_matches()
                    
                    all_matches.append(batch_matches)
                    
                    batch_progress.progress((batch_idx + 1) / len(live_batches))
                    
                    del model, live_data
                    gc.collect()
                    
                    if self.get_memory_usage() > self.max_memory_usage:
                        st.warning("‚ö†Ô∏è Memoria quasi esaurita, consolidando batch...")
                        break
                
                df_match = pd.concat(all_matches, ignore_index=True)
                del all_matches
                
            else:
                live_data = list(df_live[col].astype(str))
                staging_data = list(df_staging[col].astype(str))
                
                model = PolyFuzz("TF-IDF").match(live_data, staging_data)
                df_match = model.get_matches()
                
                del model, live_data, staging_data
            
            similarity_col = f"{col} Similarity"
            from_col = f"From ({col})"
            to_col = f"To {col}"
            
            df_match.rename(columns={
                "Similarity": similarity_col,
                "From": from_col,
                "To": to_col
            }, inplace=True)
            
            matches[col] = df_match
            
            main_progress.progress((i + 1) / total_operations)
            
            gc.collect()
            
            memory_usage = self.get_memory_usage()
            if memory_usage > 80:
                st.warning(f"‚ö†Ô∏è Utilizzo memoria: {memory_usage:.1f}%")
        
        status_text.text("‚úÖ Matching completato!")
        return matches
    
    def ai_enhance_matching(self, source_url: str, candidate_urls: List[str], 
                           source_content: Dict, candidate_contents: List[Dict]) -> Optional[str]:
        """Use OpenAI to enhance matching when similarity is low"""
        if not self.openai_client:
            return None
        
        try:
            prompt = f"""
            Sei un esperto SEO che deve identificare il miglior match per una URL di migrazione.
            
            URL di origine: {source_url}
            Contenuto origine: {source_content}
            
            Candidate URLs e contenuti:
            """
            
            for i, (url, content) in enumerate(zip(candidate_urls, candidate_contents)):
                prompt += f"\n{i+1}. URL: {url}\n   Contenuto: {content}\n"
            
            prompt += """
            
            Considera questi fattori:
            1. Similarit√† semantica del contenuto
            2. Struttura dell'URL
            3. Gerarchia del sito
            4. Intento della pagina
            
            Rispondi SOLO con il numero (1, 2, 3, etc.) della migliore opzione, o 0 se nessuna √® appropriata.
            """
            
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50,
                temperature=0.1
            )
            
            result = response.choices[0].message.content.strip()
            
            match = re.search(r'\d+', result)
            if match:
                choice = int(match.group())
                if 1 <= choice <= len(candidate_urls):
                    return candidate_urls[choice - 1]
            
            return None
            
        except Exception as e:
            st.warning(f"Errore AI enhancement: {str(e)}")
            return None
    
    def merge_matches_batch(self, df_live: pd.DataFrame, df_staging: pd.DataFrame, 
                           matches: Dict[str, pd.DataFrame], matching_columns: List[str]) -> pd.DataFrame:
        """Merge all matches into final dataframe with memory optimization"""
        
        # Ensure unique columns before starting
        df_live = self.ensure_unique_columns(df_live, "merge_matches_batch - df_live")
        df_staging = self.ensure_unique_columns(df_staging, "merge_matches_batch - df_staging")
        
        # Start with a base dataframe - only select columns that exist
        base_columns = ['Address', 'Status Code']
        available_columns = [col for col in base_columns if col in df_live.columns]
        available_matching_columns = [col for col in matching_columns if col in df_live.columns]
        
        df_final = df_live[available_columns + available_matching_columns].copy()
        
        # Ensure df_final has unique columns
        df_final = self.ensure_unique_columns(df_final, "df_final initial")
        
        # Add Address matching if available
        if 'Address' in matches and 'Address' in df_final.columns:
            try:
                # Double check that Address column is unique in df_final
                if df_final.columns.duplicated().any():
                    df_final = self.ensure_unique_columns(df_final, "before Address merge")
                
                # Perform the merge with explicit error handling
                df_final = pd.merge(df_final, matches['Address'], 
                                  left_on="Address", right_on="From (Address)", 
                                  how="left", suffixes=('', '_url_match'))
                
                # Rename the URL column
                if "To Address" in df_final.columns:
                    df_final.rename(columns={"To Address": "URL - URL Match"}, inplace=True)
                
            except ValueError as e:
                st.error(f"Errore nel merge Address: {str(e)}")
                st.write("Colonne df_final:", list(df_final.columns))
                st.write("Colonne matches['Address']:", list(matches['Address'].columns))
                # Skip Address matching if there's an error
        
        # Clean any duplicate columns that might have been created
        df_final = self.ensure_unique_columns(df_final, "after Address merge")
        
        # Merge other columns in batches to manage memory
        for col in matching_columns:
            if col == 'Address':
                continue
                
            if col in matches and col in df_staging.columns:
                try:
                    # Create mapping dataframe
                    df_col_map = df_staging[[col, 'Address']].drop_duplicates(col)
                    
                    # Rename Address column to avoid conflicts
                    df_col_map.rename(columns={'Address': f'TargetURL_{col}'}, inplace=True)
                    
                    # Merge with matches
                    df_match_with_url = pd.merge(
                        matches[col], df_col_map, 
                        left_on=f"To {col}", right_on=col, 
                        how="left"
                    )
                    
                    # Clean duplicates
                    df_match_with_url = df_match_with_url.drop_duplicates(f"From ({col})")
                    
                    # Ensure unique columns before merge
                    df_final = self.ensure_unique_columns(df_final, f"before {col} merge")
                    df_match_with_url = self.ensure_unique_columns(df_match_with_url, f"{col} match data")
                    
                    # Merge with final dataframe
                    df_final = df_final.merge(
                        df_match_with_url, 
                        how='left', left_on=col, right_on=f"From ({col})",
                        suffixes=('', f'_{col.lower().replace(" ", "_").replace("-", "_")}')
                    )
                    
                    # Rename URL column correctly
                    if f'TargetURL_{col}' in df_final.columns:
                        df_final.rename(columns={f'TargetURL_{col}': f"URL - {col} Match"}, inplace=True)
                    
                    # Clean duplicate columns after each merge
                    df_final = self.ensure_unique_columns(df_final, f"after {col} merge")
                    
                    # Memory cleanup
                    del df_col_map, df_match_with_url
                    gc.collect()
                    
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Errore nel merge della colonna {col}: {str(e)}. Saltata.")
                    continue
        
        return df_final
    
    def calculate_best_matches_batch(self, df_final: pd.DataFrame, matching_columns: List[str]) -> pd.DataFrame:
        """Calculate best, second best, and lowest matches with memory optimization"""
        
        similarity_cols = [f"{col} Similarity" for col in matching_columns if f"{col} Similarity" in df_final.columns]
        
        if not similarity_cols:
            return df_final
        
        chunk_size = 10000
        if len(df_final) > chunk_size:
            st.write(f"üìä Calcolo match in chunks di {chunk_size:,} righe...")
            
            chunks_processed = []
            total_chunks = math.ceil(len(df_final) / chunk_size)
            
            for i in range(0, len(df_final), chunk_size):
                chunk = df_final.iloc[i:i+chunk_size].copy()
                chunk = self._calculate_matches_for_chunk(chunk, similarity_cols, matching_columns)
                chunks_processed.append(chunk)
                
                if (i // chunk_size + 1) % 5 == 0:
                    st.write(f"   Processati {i // chunk_size + 1}/{total_chunks} chunks...")
            
            df_final = pd.concat(chunks_processed, ignore_index=True)
            del chunks_processed
            gc.collect()
        else:
            df_final = self._calculate_matches_for_chunk(df_final, similarity_cols, matching_columns)
        
        return df_final
    
    def _calculate_matches_for_chunk(self, chunk: pd.DataFrame, similarity_cols: List[str], 
                                   matching_columns: List[str]) -> pd.DataFrame:
        """Calculate matches for a single chunk"""
        
        chunk['Best Match On'] = chunk[similarity_cols].idxmax(axis=1)
        chunk['Highest Match Similarity'] = chunk[similarity_cols].max(axis=1)
        
        chunk['Lowest Match On'] = chunk[similarity_cols].idxmin(axis=1)
        chunk['Lowest Match Similarity'] = chunk[similarity_cols].min(axis=1)
        
        def get_second_best(row):
            values = [(col, row[col]) for col in similarity_cols if pd.notna(row[col])]
            if len(values) < 2:
                return None, None
            values.sort(key=lambda x: x[1], reverse=True)
            return values[1] if len(values) > 1 else (None, None)
        
        second_matches = chunk.apply(get_second_best, axis=1, result_type='expand')
        chunk['Second Match On'] = second_matches[0]
        chunk['Second Highest Match Similarity'] = second_matches[1]
        
        for col in matching_columns:
            similarity_col = f"{col} Similarity"
            url_col = f"URL - {col} Match"
            from_col = f"From ({col})"
            to_col = f"To {col}"
            
            if similarity_col in chunk.columns:
                mask = chunk['Best Match On'] == similarity_col
                if url_col in chunk.columns:
                    chunk.loc[mask, 'Best Matching URL'] = chunk.loc[mask, url_col]
                if from_col in chunk.columns:
                    chunk.loc[mask, 'Highest Match Source Text'] = chunk.loc[mask, from_col]
                if to_col in chunk.columns:
                    chunk.loc[mask, 'Highest Match Destination Text'] = chunk.loc[mask, to_col]
                
                mask = chunk['Second Match On'] == similarity_col
                if url_col in chunk.columns:
                    chunk.loc[mask, 'Second Highest Match'] = chunk.loc[mask, url_col]
                if from_col in chunk.columns:
                    chunk.loc[mask, 'Second Match Source Text'] = chunk.loc[mask, from_col]
                if to_col in chunk.columns:
                    chunk.loc[mask, 'Second Match Destination Text'] = chunk.loc[mask, to_col]
        
        if 'Best Matching URL' in chunk.columns and 'Second Highest Match' in chunk.columns:
            chunk["Double Matched?"] = (
                chunk['Best Matching URL'].str.lower() == 
                chunk['Second Highest Match'].str.lower()
            )
        
        chunk['Best Match On'] = chunk['Best Match On'].str.replace(' Similarity', '')
        if 'Second Match On' in chunk.columns:
            chunk['Second Match On'] = chunk['Second Match On'].str.replace(' Similarity', '')
        chunk['Lowest Match On'] = chunk['Lowest Match On'].str.replace(' Similarity', '')
        
        return chunk
    
    def apply_ai_enhancement_batch(self, df_final: pd.DataFrame, max_calls: int) -> pd.DataFrame:
        """Apply AI enhancement in controlled batches"""
        
        low_similarity_mask = df_final['Highest Match Similarity'] < 0.7
        low_similarity_indices = df_final[low_similarity_mask].index[:max_calls]
        
        progress_bar = st.progress(0)
        enhanced_count = 0
        
        batch_size = 5
        for i in range(0, len(low_similarity_indices), batch_size):
            batch_indices = low_similarity_indices[i:i+batch_size]
            
            for idx in batch_indices:
                try:
                    source_url = df_final.loc[idx, 'Address']
                    
                    candidates = []
                    url_cols = [col for col in df_final.columns if col.startswith('URL -') and 'Match' in col]
                    
                    for url_col in url_cols:
                        if pd.notna(df_final.loc[idx, url_col]):
                            candidates.append(df_final.loc[idx, url_col])
                    
                    if candidates:
                        ai_match = self.ai_enhance_matching(
                            source_url, candidates[:3], {}, [{}]*len(candidates[:3])
                        )
                        
                        if ai_match:
                            df_final.loc[idx, 'Best Matching URL'] = ai_match
                            df_final.loc[idx, 'Best Match On'] = 'AI Enhanced'
                            df_final.loc[idx, 'Highest Match Similarity'] = 0.95
                            enhanced_count += 1
                
                except Exception as e:
                    continue
            
            progress_bar.progress(min((i + batch_size) / len(low_similarity_indices), 1.0))
            time.sleep(0.1)
        
        st.write(f"ü§ñ AI enhancement applicato a {enhanced_count} URL")
        return df_final
    
    def finalize_results_batch(self, df_final: pd.DataFrame) -> pd.DataFrame:
        """Finalize results with memory optimization"""
        
        initial_count = len(df_final)
        df_final.drop_duplicates(subset="Address", inplace=True)
        duplicates_removed = initial_count - len(df_final)
        
        if duplicates_removed > 0:
            st.write(f"üóëÔ∏è Rimossi {duplicates_removed} duplicati finali")
        
        base_columns = [
            "Address", "Status Code", "Best Matching URL", "Best Match On", 
            "Highest Match Similarity"
        ]
        
        optional_columns = [
            "Highest Match Source Text", "Highest Match Destination Text", 
            "Second Highest Match", "Second Match On", "Second Highest Match Similarity", 
            "Second Match Source Text", "Second Match Destination Text", "Double Matched?"
        ]
        
        final_columns = [col for col in base_columns if col in df_final.columns]
        final_columns.extend([col for col in optional_columns if col in df_final.columns])
        
        df_final = df_final[final_columns]
        
        df_final.rename(columns={"Address": "URL - Source"}, inplace=True)
        
        if 'Highest Match Similarity' in df_final.columns:
            df_final = df_final.sort_values(["Highest Match Similarity"], ascending=[False], na_position='last')
        
        return df_final
    
    def process_migration_batch(self, df_live: pd.DataFrame, df_staging: pd.DataFrame, 
                              matching_columns: List[str], use_ai: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Main processing function with batch optimization"""
        
        start_time = time.time()
        
        initial_memory = self.get_memory_usage()
        st.write(f"üíæ Memoria iniziale: {initial_memory:.1f}%")
        
        total_rows = len(df_live) + len(df_staging)
        st.write(f"üìä Righe totali da processare: {total_rows:,}")
        
        if total_rows > 50000:
            st.info("üîÑ Dataset grande rilevato. Utilizzo elaborazione ottimizzata a batch...")
            
        df_live_clean, df_staging_clean, df_non_redirectable = self.preprocess_data_batch(
            df_live, df_staging, matching_columns
        )
        
        matches = self.perform_polyfuzz_matching_batch(df_live_clean, df_staging_clean, matching_columns)
        
        st.write("üîó Merge dei risultati...")
        df_final = self.merge_matches_batch(df_live_clean, df_staging_clean, matches, matching_columns)
        
        st.write("üéØ Calcolo dei migliori match...")
        df_final = self.calculate_best_matches_batch(df_final, matching_columns)
        
        if use_ai and self.openai_client and 'Highest Match Similarity' in df_final.columns:
            total_low_similarity = (df_final['Highest Match Similarity'] < 0.7).sum()
            
            if total_low_similarity > 0:
                max_ai_calls = min(100, total_low_similarity) if total_rows > 10000 else total_low_similarity
                
                st.write(f"ü§ñ Applicazione AI a {max_ai_calls} match con bassa similarit√† (di {total_low_similarity})...")
                
                df_final = self.apply_ai_enhancement_batch(df_final, max_ai_calls)
        
        df_final = self.finalize_results_batch(df_final)
        
        del df_live_clean, df_staging_clean, matches
        gc.collect()
        
        processing_time = time.time() - start_time
        final_memory = self.get_memory_usage()
        
        st.success(f"‚úÖ Processamento completato in {processing_time:.2f} secondi!")
        st.write(f"üíæ Memoria finale: {final_memory:.1f}% (Œî: {final_memory - initial_memory:+.1f}%)")
        
### 5. **Ottimizzazione tipi di dati**
- üìä **Category dtype** per colonne con molti valori ripetuti
- üìä **String dtype** ottimizzato per testo
- üìä **Memory usage reporting** per monitorare consumo
- üìä **Pulizia intelligente** righe e colonne vuote

## üöÄ **Come funziona ora:**

### **Caricamento resiliente:**
```
1. Tenta encoding UTF-8
2. Se fallisce ‚Üí Latin-1, CP1252  
3. Skip righe problematiche automaticamente
4. Chunks pi√π piccoli per file giganti
5. Monitor memoria continuo
```

### **Elaborazione sicura:**
```
1. Health check prima di ogni step
2. Stop automatico se memoria critica
3. Garbage collection frequente
4. Skip AI se risorse insufficienti
5. Fallback a operazioni semplificate
```

### **Prevenzione crash:**
```
1. Wrapper sicurezza su tutte le funzioni
2. Monitor memoria real-time  
3. Cleanup automatico intermedio
4. Error handling specifico
5. Batch size adattivo
```

## üéØ **Benefici specifici per il tuo caso:**

### ‚úÖ **File Screaming Frog corrotti:**
- **Righe inconsistenti** ‚Üí Saltate automaticamente
- **Colonne extra** ‚Üí Gestite senza crash
- **Encoding problemi** ‚Üí Risolti con fallback
- **File giganti** ‚Üí Processati in sicurezza

### ‚úÖ **Prevenzione crash:**
- **Memoria critica** ‚Üí Stop sicuro prima del crash
- **Righe problematiche** ‚Üí Skip instead di fail
- **Batch overflow** ‚Üí Riduzione automatica size
- **Memory leaks** ‚Üí Cleanup forzato

### ‚úÖ **Feedback migliorato:**
- **Progress real-time** con stato memoria
- **Diagnostica specifica** per ogni tipo errore
- **Suggerimenti** automatici per risolvere problemi
- **Memory usage** reporting dettagliato

## üìã **Parametri ottimizzati per file grandi:**

```python
# Per file 200K+ righe:
batch_size = 3000 (vs 5000 default)
ai_calls = 50 (vs 100 default)
memory_threshold = 90% (vs 85% default)
chunk_size = 5000 (vs 10000 default)

# Monitoring:
health_check = every_operation
garbage_collection = every_10_chunks  
memory_reporting = real_time
```

## üîß **Cosa fare se l'app √® ancora lenta:**

### **Ottimizzazioni ulteriori:**
1. **Riduci colonne matching** - Usa solo Address + 1-2 colonne essenziali
2. **Disabilita AI** per file >100K righe  
3. **Aumenta memoria sistema** se possibile
4. **Processa in pi√π sessioni** dividendo il file

### **Preparazione file:**
1. **Pulisci in Excel** prima del caricamento
2. **Rimuovi colonne inutili** per ridurre size
3. **Salva come CSV UTF-8** per encoding consistente
4. **Dividi file enormi** in parti <100K righe

## üéâ **Risultato finale:**

Ora l'app dovrebbe:
- ‚úÖ **Non crashare mai** per problemi di memoria
- ‚úÖ **Gestire file corrotti** di Screaming Frog  
- ‚úÖ **Processare 290K+ righe** in sicurezza
- ‚úÖ **Fornire feedback** dettagliato sui problemi
- ‚úÖ **Completare elaborazione** anche con dati problematici

Il tool √® ora **production-ready** per gestire qualsiasi export di Screaming Frog, anche con righe inconsistenti e dimensioni enormi! üöÄ

**Prova il nuovo sistema** - dovrebbe gestire perfettamente anche i file pi√π problematici senza crash! üòä
    
    def process_migration(self, df_live: pd.DataFrame, df_staging: pd.DataFrame, 
                         matching_columns: List[str], use_ai: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Main processing function - delegates to batch version for large datasets"""
        
        return self.process_migration_batch(df_live, df_staging, matching_columns, use_ai)

# Streamlit App
def main():
    st.title("üîó URL Migration Mapping Tool")
    st.markdown("**Strumento avanzato per il mapping automatico di URL durante le migrazioni di siti web**")
    st.markdown("*Ottimizzato per file di grandi dimensioni (fino a 1GB)*")
    
    mapper = URLMigrationMapper()
    
    st.sidebar.header("‚öôÔ∏è Configurazione")
    
    st.sidebar.subheader("üíæ Info Sistema")
    available_memory = psutil.virtual_memory().available / (1024**3)
    total_memory = psutil.virtual_memory().total / (1024**3)
    st.sidebar.write(f"Memoria disponibile: {available_memory:.1f} GB / {total_memory:.1f} GB")
    st.sidebar.write(f"Utilizzo corrente: {psutil.virtual_memory().percent:.1f}%")
    
    st.sidebar.subheader("üöÄ Impostazioni Performance")
    
    auto_batch = st.sidebar.checkbox("Auto-batch size", value=True, 
                                    help="Calcola automaticamente la dimensione ottimale del batch")
    
    if not auto_batch:
        custom_batch_size = st.sidebar.slider("Batch size personalizzato", 
                                             min_value=1000, max_value=20000, 
                                             value=5000, step=1000,
                                             help="Dimensione del batch per l'elaborazione")
        mapper.batch_size = custom_batch_size
    
    memory_limit = st.sidebar.slider("Limite memoria (%)", 
                                    min_value=70, max_value=95, 
                                    value=85, step=5,
                                    help="Limite massimo di utilizzo memoria prima dell'ottimizzazione")
    mapper.max_memory_usage = memory_limit
    
    st.sidebar.subheader("ü§ñ Configurazione OpenAI (Opzionale)")
    use_ai = st.sidebar.checkbox("Abilita miglioramento AI", 
                                help="Migliora i match con bassa similarit√† usando OpenAI GPT")
    
    api_key = None
    if use_ai:
        api_key = st.sidebar.text_input("API Key OpenAI", type="password", 
                                       help="Inserisci la tua API key OpenAI per abilitare il miglioramento AI")
        
        ai_limit = st.sidebar.number_input("Limite chiamate AI", 
                                          min_value=10, max_value=1000, 
                                          value=100, step=10,
                                          help="Numero massimo di chiamate AI per dataset grandi")
        
        if api_key:
            if mapper.setup_openai(api_key):
                st.sidebar.success("‚úÖ OpenAI configurato correttamente")
            else:
                use_ai = False
    
    st.header("üìÅ Caricamento File")
    st.markdown("*Supporta file fino a 1GB (CSV, XLSX, XLS)*")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("File PRE-migrazione (Live)")
        pre_file = st.file_uploader(
            "Carica file pre-migrazione", 
            type=['csv', 'xlsx', 'xls'],
            key="pre_file",
            help="File contenente le URL del sito live prima della migrazione (max 1GB)"
        )
    
    with col2:
        st.subheader("File POST-migrazione (Staging)")
        post_file = st.file_uploader(
            "Carica file post-migrazione", 
            type=['csv', 'xlsx', 'xls'],
            key="post_file",
            help="File contenente le URL del sito staging dopo la migrazione (max 1GB)"
        )
    
    if pre_file and post_file:
        pre_size_mb = pre_file.size / (1024 * 1024)
        post_size_mb = post_file.size / (1024 * 1024)
        
        col1, col2 = st.columns(2)
        with col1:
            if pre_size_mb > 100:
                st.warning(f"‚ö†Ô∏è File grande: {pre_size_mb:.1f} MB")
            else:
                st.info(f"üìä Dimensione: {pre_size_mb:.1f} MB")
        
        with col2:
            if post_size_mb > 100:
                st.warning(f"‚ö†Ô∏è File grande: {post_size_mb:.1f} MB")
            else:
                st.info(f"üìä Dimensione: {post_size_mb:.1f} MB")
        
        with st.spinner("Caricamento file..."):
            df_pre = mapper.load_file(pre_file, 'pre')
            df_post = mapper.load_file(post_file, 'post')
        
        if df_pre is not None and df_post is not None:
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("File PRE", f"{len(df_pre):,} righe", f"{len(df_pre.columns)} colonne")
            with col2:
                st.metric("File POST", f"{len(df_post):,} righe", f"{len(df_post.columns)} colonne")
            with col3:
                total_combinations = len(df_pre) * len(df_post)
                if total_combinations > 1e9:
                    st.metric("Complessit√†", "Molto Alta", "üî¥")
                elif total_combinations > 1e7:
                    st.metric("Complessit√†", "Alta", "üü°")
                else:
                    st.metric("Complessit√†", "Media", "üü¢")
            
            estimated_memory = (len(df_pre) + len(df_post)) * len(df_pre.columns) * 0.001
            if estimated_memory > available_memory * 1024 * 0.8:
                st.error("‚ö†Ô∏è Il dataset potrebbe superare la memoria disponibile. Considera di ridurre le dimensioni del file.")
            elif estimated_memory > available_memory * 1024 * 0.5:
                st.warning("‚ö†Ô∏è Dataset grande rilevato. L'elaborazione utilizzer√† la modalit√† batch ottimizzata.")
            
            st.header("üéØ Selezione Colonne per Matching")
            
            common_columns = list(set(df_pre.columns) & set(df_post.columns))
            
            required_pre = ['Address', 'Status Code']
            required_post = ['Address', 'Status Code', 'Indexability']
            
            missing_pre = [col for col in required_pre if col not in df_pre.columns]
            missing_post = [col for col in required_post if col not in df_post.columns]
            
            if missing_pre or missing_post:
                st.error("‚ùå Colonne obbligatorie mancanti:")
                if missing_pre:
                    st.error(f"File PRE: {missing_pre}")
                if missing_post:
                    st.error(f"File POST: {missing_post}")
                return
            
            matching_options = [col for col in common_columns if col not in ['Status Code', 'Indexability']]
            
            default_matching = ['Address']
            if 'Title 1' in matching_options:
                default_matching.append('Title 1')
            if 'H1-1' in matching_options:
                default_matching.append('H1-1')
            
            total_rows = len(df_pre) + len(df_post)
            if total_rows > 100000:
                st.info("üí° **Consiglio per dataset grandi:** Limita il numero di colonne di matching (max 3-4) per ottimizzare le performance.")
            
            matching_columns = st.multiselect(
                "Seleziona colonne per il matching:",
                options=matching_options,
                default=default_matching,
                help="Seleziona le colonne su cui basare il matching delle URL. Pi√π colonne = matching pi√π accurato ma elaborazione pi√π lenta."
            )
            
            if not matching_columns:
                st.warning("‚ö†Ô∏è Seleziona almeno una colonna per il matching")
                return
            
            if len(matching_columns) > 5 and total_rows > 50000:
                st.warning("‚ö†Ô∏è Molte colonne selezionate con dataset grande. L'elaborazione potrebbe richiedere molto tempo.")
            
            if st.checkbox("üëÄ Anteprima dati"):
                preview_rows = min(5, len(df_pre), len(df_post))
                col1, col2 = st.columns(2)
                with col1:
                    st.write(f"**Anteprima PRE (prime {preview_rows} righe):**")
                    display_cols = matching_columns + ['Status Code']
                    st.dataframe(df_pre[display_cols].head(preview_rows))
                
                with col2:
                    st.write(f"**Anteprima POST (prime {preview_rows} righe):**")
                    display_cols = matching_columns + ['Status Code', 'Indexability']
                    display_cols = [col for col in display_cols if col in df_post.columns]
                    st.dataframe(df_post[display_cols].head(preview_rows))
            
            st.header("üöÄ Elaborazione")
            
            if total_rows > 10000:
                estimated_time = (total_rows / 10000) * len(matching_columns) * 30
                st.info(f"‚è±Ô∏è Tempo stimato: {estimated_time/60:.1f} minuti (per {total_rows:,} righe totali)")
            
            col1, col2 = st.columns(2)
            with col1:
                process_button = st.button("‚ñ∂Ô∏è Avvia Mapping", type="primary", use_container_width=True)
            with col2:
                if st.button("üóëÔ∏è Libera Memoria", help="Forza garbage collection per liberare memoria"):
                    gc.collect()
                    st.success("Memoria liberata!")
                    st.rerun()
            
            if process_button:
                try:
                    # Pre-flight checks
                    current_memory = psutil.virtual_memory().percent
                    available_memory = psutil.virtual_memory().available / (1024**3)
                    
                    if current_memory > 90:
                        st.error(f"üö® MEMORIA TROPPO ALTA: {current_memory:.1f}%")
                        st.error("Libera memoria prima di procedere (chiudi altre applicazioni)")
                        return
                    
                    if available_memory < 2:
                        st.error(f"üö® MEMORIA DISPONIBILE INSUFFICIENTE: {available_memory:.1f} GB")
                        st.error("Servono almeno 2GB di memoria disponibile per file grandi")
                        return
                    
                    # Estimate resource requirements
                    estimated_memory_gb = (len(df_pre) + len(df_post)) * len(matching_columns) * 0.000001
                    if estimated_memory_gb > available_memory * 0.8:
                        st.warning(f"‚ö†Ô∏è ATTENZIONE: Elaborazione richieder√† ~{estimated_memory_gb:.1f}GB")
                        st.warning("Considera di ridurre il numero di colonne di matching o dividere il file")
                        
                        # Ask for confirmation
                        if not st.button("üöÄ Procedi comunque (RISCHIO CRASH)", type="secondary"):
                            st.info("üí° Riduci le colonne di matching o libera pi√π memoria prima di procedere")
                            return
                    
                    # Dynamic configuration based on file size
                    total_rows = len(df_pre) + len(df_post)
                    
                    if total_rows > 500000:
                        st.warning("üîß Dataset MOLTO GRANDE: Applicando configurazione ultra-conservativa")
                        mapper.batch_size = 2000
                        mapper.max_memory_usage = 75
                        use_ai = False  # Force disable AI for huge datasets
                        st.info("ü§ñ AI disabilitato automaticamente per dataset gigante")
                    elif total_rows > 200000:
                        st.info("üîß Dataset GRANDE: Applicando configurazione conservativa")
                        mapper.batch_size = 3000
                        mapper.max_memory_usage = 80
                    
                    # Show final configuration
                    st.info(f"‚öôÔ∏è Configurazione: Batch={mapper.batch_size}, Memory={mapper.max_memory_usage}%, AI={use_ai and api_key is not None}")
                    
                    with st.spinner("Elaborazione in corso..."):
                        # Create a placeholder for real-time updates
                        status_placeholder = st.empty()
                        
                        def update_status():
                            mem = psutil.virtual_memory().percent
                            status_placeholder.info(f"üìä Elaborazione in corso... Memoria: {mem:.1f}%")
                        
                        # Update status periodically
                        update_status()
                        
                        start_time = time.time()
                        df_result, df_non_redirectable = mapper.process_migration(
                            df_pre, df_post, matching_columns, use_ai and api_key is not None
                        )
                        end_time = time.time()
                        
                        status_placeholder.empty()
                    
                    # Verify we got results
                    if df_result is None or len(df_result) == 0:
                        st.error("‚ùå Nessun risultato prodotto. Verifica i dati di input.")
                        return
                    
                    st.header("üìä Risultati")
                    
                    # Performance stats
                    processing_time = end_time - start_time
                    rows_per_second = len(df_result) / processing_time if processing_time > 0 else 0
                    
                    col1, col2, col3, col4, col5 = st.columns(5)
                    with col1:
                        st.metric("URL Processate", f"{len(df_result):,}")
                    with col2:
                        st.metric("Non-redirectable", f"{len(df_non_redirectable):,}")
                    with col3:
                        st.metric("Tempo Elaborazione", f"{processing_time:.1f}s")
                    with col4:
                        st.metric("Velocit√†", f"{rows_per_second:.0f} righe/s")
                    with col5:
                        if 'Highest Match Similarity' in df_result.columns:
                            avg_similarity = df_result['Highest Match Similarity'].mean()
                            st.metric("Similarit√† Media", f"{avg_similarity:.3f}")
                    
                    # Quality metrics
                    if 'Highest Match Similarity' in df_result.columns:
                        similarity_data = df_result['Highest Match Similarity'].dropna()
                        if len(similarity_data) > 0:
                            high_quality = (similarity_data >= 0.8).sum()
                            medium_quality = ((similarity_data >= 0.5) & (similarity_data < 0.8)).sum()
                            low_quality = (similarity_data < 0.5).sum()
                            
                            st.subheader("üìà Analisi Qualit√†")
                            qual_col1, qual_col2, qual_col3 = st.columns(3)
                            with qual_col1:
                                st.metric("Alta Qualit√†", f"{high_quality:,}", f"{high_quality/len(similarity_data)*100:.1f}%")
                            with qual_col2:
                                st.metric("Media Qualit√†", f"{medium_quality:,}", f"{medium_quality/len(similarity_data)*100:.1f}%")
                            with qual_col3:
                                st.metric("Bassa Qualit√†", f"{low_quality:,}", f"{low_quality/len(similarity_data)*100:.1f}%")
                    
                    # Show results table (limited for large datasets)
                    st.subheader("üéØ Risultati Mapping")
                    
                    display_limit = 1000 if len(df_result) > 1000 else len(df_result)
                    if len(df_result) > display_limit:
                        st.info(f"Mostra prime {display_limit} righe di {len(df_result):,} risultati totali")
                        st.dataframe(df_result.head(display_limit), use_container_width=True, height=400)
                    else:
                        st.dataframe(df_result, use_container_width=True, height=400)
                    
                    # Download section
                    st.header("üíæ Download Risultati")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # Estimate CSV size
                        estimated_size_mb = len(df_result) * len(df_result.columns) * 50 / (1024 * 1024)
                        
                        # Generate CSV with progress for large files
                        if len(df_result) > 50000:
                            with st.spinner("Generazione CSV..."):
                                csv_result = df_result.to_csv(index=False)
                        else:
                            csv_result = df_result.to_csv(index=False)
                        
                        st.download_button(
                            label=f"üì• Scarica Mapping Completo (CSV) ~{estimated_size_mb:.1f}MB",
                            data=csv_result,
                            file_name=f"auto-migration-mapped-{len(df_result)}-urls-{int(time.time())}.csv",
                            mime="text/csv",
                            use_container_width=True
                        )
                    
                    with col2:
                        # Non-redirectable URLs
                        if len(df_non_redirectable) > 0:
                            csv_non_redirect = df_non_redirectable.to_csv(index=False)
                            st.download_button(
                                label=f"üì• URL Non-redirectable (CSV) - {len(df_non_redirectable)} righe",
                                data=csv_non_redirect,
                                file_name=f"auto-migration-non-redirectable-{int(time.time())}.csv",
                                mime="text/csv",
                                use_container_width=True
                            )
                        else:
                            st.info("‚úÖ Nessuna URL non-redirectable trovata")
                    
                    # Final cleanup
                    del df_result, df_non_redirectable, csv_result
                    if 'csv_non_redirect' in locals():
                        del csv_non_redirect
                    gc.collect()
                    
                    # Final memory status
                    final_memory = psutil.virtual_memory().percent
                    st.success(f"üéâ Elaborazione completata! Memoria finale: {final_memory:.1f}%")
                
                except MemoryError as e:
                    st.error("‚ùå MEMORIA INSUFFICIENTE")
                    st.error("Il sistema ha esaurito la memoria disponibile durante l'elaborazione")
                    st.markdown("""
                    **üîß Soluzioni immediate:**
                    1. **Chiudi altre applicazioni** per liberare memoria
                    2. **Riduci colonne matching** (usa solo Address + 1 colonna)
                    3. **Dividi il file** in parti pi√π piccole (<100K righe)
                    4. **Riavvia browser** per liberare memoria cache
                    5. **Usa computer con pi√π RAM** se disponibile
                    """)
                    gc.collect()
                
                except Exception as e:
                    st.error(f"‚ùå Errore durante l'elaborazione: {str(e)}")
                    
                    # Provide context-specific help
                    error_str = str(e).lower()
                    if "column" in error_str and "unique" in error_str:
                        st.markdown("""
                        **üîß Problema: Colonne duplicate rilevate**
                        - Il file contiene colonne con nomi identici
                        - Apri il file in Excel e verifica i nomi delle colonne
                        - Rinomina eventuali colonne duplicate
                        """)
                    elif "memory" in error_str:
                        st.markdown("""
                        **üîß Problema: Memoria insufficiente**
                        - Chiudi altre applicazioni
                        - Riduci il numero di colonne di matching
                        - Dividi il file in parti pi√π piccole
                        """)
                    elif "encoding" in error_str:
                        st.markdown("""
                        **üîß Problema: Encoding del file**
                        - Salva il file come CSV UTF-8 da Excel
                        - Verifica che non ci siano caratteri speciali
                        """)
                    else:
                        st.markdown("""
                        **üîß Suggerimenti generali:**
                        - Verifica che entrambi i file abbiano le colonne richieste
                        - Controlla che i dati siano nel formato corretto
                        - Prova con un file di test pi√π piccolo
                        """)
                    
                    # Cleanup on error
                    gc.collect()
    
    with st.expander("‚ÑπÔ∏è Informazioni e Ottimizzazioni"):
        st.markdown("""
        ### üöÄ Ottimizzazioni per File Grandi:
        
        **Nuove Funzionalit√† v2.0:**
        - ‚úÖ **Elaborazione Batch**: Processamento automatico in chunks per file fino a 1GB
        - ‚úÖ **Monitoraggio Memoria**: Controllo real-time dell'utilizzo memoria con auto-ottimizzazione
        - ‚úÖ **Batch Size Dinamico**: Calcolo automatico della dimensione ottimale del batch
        - ‚úÖ **AI Limitato**: Per dataset grandi, limita le chiamate AI per controllare i costi
        - ‚úÖ **Progress Tracking**: Barre di progresso dettagliate per ogni fase
        - ‚úÖ **Garbage Collection**: Pulizia automatica della memoria durante l'elaborazione
        
        ### üìä Raccomandazioni per Performance:
        
        **File < 10K righe:** Elaborazione standard, tutte le funzionalit√† disponibili  
        **File 10K-100K righe:** Modalit√† batch automatica, AI limitato  
        **File > 100K righe:** Batch ottimizzato, AI solo per casi critici  
        **File > 500K righe:** Limita colonne matching (max 3-4), disabilita anteprima  
        
        ### üíæ Gestione Memoria:
        
        - **Auto-batch**: Il sistema calcola automaticamente la dimensione ottimale
        - **Limite memoria**: Impostabile da 70% a 95% (default 85%)
        - **Cleanup automatico**: Garbage collection tra le fasi di elaborazione
        - **Monitoraggio real-time**: Visualizzazione utilizzo memoria durante l'elaborazione
        
        ### ‚ö° Tips per Ottimizzare:
        
        1. **Usa meno colonne** per dataset molto grandi (Address + 1-2 colonne aggiuntive)
        2. **Disabilita l'AI** per file > 200K righe per velocizzare l'elaborazione
        3. **Aumenta il limite memoria** se hai RAM sufficiente
        4. **Processa in pi√π sessioni** dividendo file enormi
        5. **Usa file CSV** invece di Excel quando possibile (pi√π veloce)
        
        ### üîß Colonne richieste:
        
        **File PRE:** Address, Status Code  
        **File POST:** Address, Status Code, Indexability
        
        ### üéØ Algoritmo Ottimizzato:
        
        1. **Caricamento Chunk-based** per file grandi
        2. **Preprocessing con ottimizzazione tipi dati**
        3. **Matching batch-wise** con PolyFuzz
        4. **Merge incrementale** per gestire memoria
        5. **AI enhancement controllato** per qualit√†
        6. **Export ottimizzato** con compressione
        """)

if __name__ == "__main__":
    main()
